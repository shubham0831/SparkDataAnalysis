{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03cb4dd7",
   "metadata": {},
   "source": [
    "# Site Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371019ed",
   "metadata": {},
   "source": [
    "## Subreddit Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bff9ad",
   "metadata": {},
   "source": [
    "### How many unique subreddits were there at the beginning and end of 2018?\n",
    "\n",
    "I interpret the beginning and end of 2018 to be the first and last months of 2018. So to answer this question, I will find the number of unique subreddits which had comments posted to them in Januray 2018 and the same number for December 2018 and compare these two numbers to analyze subreddit growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a73b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan_18_res = spark.read.json('hdfs://orion11:13001/RES-RC_2018-01.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1296af81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- can_gild: boolean (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- subreddit_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_jan_18_res.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4cbabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the estimated cardinality by HyperLogLog++\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "approx_count = df_jan_18_res.select(approx_count_distinct(\"subreddit_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451e82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_approx_count_list = approx_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090bf7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_approx_count_value = jan_approx_count_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ad5888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51805"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jan_approx_count_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeb6bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dec_18_res = spark.read.json('hdfs://orion11:13001/RES-RC_2018-12.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "310bfeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_created_utc: long (nullable = true)\n",
      " |-- author_flair_background_color: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_richtext: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- a: string (nullable = true)\n",
      " |    |    |-- e: string (nullable = true)\n",
      " |    |    |-- t: string (nullable = true)\n",
      " |    |    |-- u: string (nullable = true)\n",
      " |-- author_flair_template_id: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- author_flair_text_color: string (nullable = true)\n",
      " |-- author_flair_type: string (nullable = true)\n",
      " |-- author_fullname: string (nullable = true)\n",
      " |-- author_patreon_flair: boolean (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- can_gild: boolean (nullable = true)\n",
      " |-- can_mod_post: boolean (nullable = true)\n",
      " |-- collapsed: boolean (nullable = true)\n",
      " |-- collapsed_reason: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- gildings: struct (nullable = true)\n",
      " |    |-- gid_1: long (nullable = true)\n",
      " |    |-- gid_2: long (nullable = true)\n",
      " |    |-- gid_3: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- no_follow: boolean (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- removal_reason: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- send_replies: boolean (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- subreddit_name_prefixed: string (nullable = true)\n",
      " |-- subreddit_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dec_18_res.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdeb58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_approx_count = df_dec_18_res.select(approx_count_distinct(\"subreddit_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8b9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_approx_count_list = dec_approx_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c90b0476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56457"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_approx_count_value = dec_approx_count_list[0][0]\n",
    "dec_approx_count_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8baaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4652"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_subreddit_growth = dec_approx_count_value - jan_approx_count_value\n",
    "net_subreddit_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e802f9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.979828201911012"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_subreddit_growth = (net_subreddit_growth / jan_approx_count_value) * 100\n",
    "percent_subreddit_growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3590a3cc",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "The number of subreddits grew approximately 9% in 2018. There were approximately 51805 unique subreddits in the beginning of 2018 and approximately 56457 unique subreddits at the end of 2018 in the reservoir sampled data. This is an increase of 4652 subreddits or almost 9% growth in subreddits. While this answer is approximate because I used the sampled data and used the HyperLogLog++ algorithm which provides a cardinality estimate, I believe the 9% growth in the number of subreddits is a good estimate and is more accurate than the estimated number of unique subreddits because I used the sampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b8b02",
   "metadata": {},
   "source": [
    "## User Growth\n",
    "### How many active users does Reddit have now compared to the past?\n",
    "\n",
    "We can define active users as monthly active users. We can define \"now\" as the latest month of data which is December 2020. How do we define the past? Let's look at user growth over a decade so we can define the \"the past\" as December 2010 and look at the growth in monthly active users over 10 years. Let's define a monthly active user as a Reddit account which posted at least one comment in that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c745611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dec_10 = spark.read.json('hdfs://orion11:13001/RC_2010-12.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e054cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- downs: long (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- score_hidden: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dec_10.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf362d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the schema because we are only interested in users\n",
    "\n",
    "df_dec_10_author = df_dec_10.select('author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b6825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes duplicate users\n",
    "\n",
    "df_dec_10_distinct_users = df_dec_10_author.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb2d5562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243810"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_10_users = df_dec_10_distinct_users.count()\n",
    "dec_10_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf4cd9df",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o47.json.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 64) (10.0.1.26 executor 6): java.lang.InternalError: Frame requires too much memory for decoding\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor.inflateBytesDirect(Native Method)\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor.decompress(ZStandardDecompressor.java:187)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.base/java.io.InputStream.read(InputStream.java:205)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:116)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:99)\n\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:65)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:361)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.InternalError: Frame requires too much memory for decoding\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor.inflateBytesDirect(Native Method)\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor.decompress(ZStandardDecompressor.java:187)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.base/java.io.InputStream.read(InputStream.java:205)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c7f836dc3459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_dec_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hdfs://orion11:13001/RC_2020-12.zst'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/bigdata/spark-3.3.0-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bigdata/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bigdata/spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bigdata/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o47.json.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 64) (10.0.1.26 executor 6): java.lang.InternalError: Frame requires too much memory for decoding\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor.inflateBytesDirect(Native Method)\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor.decompress(ZStandardDecompressor.java:187)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.base/java.io.InputStream.read(InputStream.java:205)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:116)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:99)\n\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:65)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:361)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.InternalError: Frame requires too much memory for decoding\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor.inflateBytesDirect(Native Method)\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor.decompress(ZStandardDecompressor.java:187)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.base/java.io.InputStream.read(InputStream.java:205)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# java.lang.InternalError: Frame requires too much memory for decoding ðŸ˜±\n",
    "\n",
    "df_dec_20 = spark.read.json('hdfs://orion11:13001/RC_2020-12.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dba01f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with the reservoir sample instead of the full month of data\n",
    "\n",
    "df_dec_20 = spark.read.json('hdfs://orion11:13001/RES-RC_2020-12.zst').select('author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b755912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dec_20_distinct_users = df_dec_20.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a1d656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2906214"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_20_users = df_dec_20_distinct_users.count()\n",
    "dec_20_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c37896f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and pasted after running `hdfs dfs -ls /`\n",
    "\n",
    "full_file_size = 19222994313\n",
    "sample_file_size = 2878295402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b41792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19409452"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_20_users_scaled = round(dec_20_users * (full_file_size / sample_file_size))\n",
    "dec_20_users_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78ea8afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19165642"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_user_growth = dec_20_users_scaled - dec_10_users\n",
    "net_user_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b4ec2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7860.89249825684"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_user_growth = (net_user_growth / dec_10_users) * 100\n",
    "percent_user_growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4e3d7",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Reddit grew in the number of monthly active users by approximately 7860% in the decade between December 2010 and December 2020. We are defining an active user as a user who posted a comment on Reddit in that month. In December 2010, 243,810 unique accounts posted a comment. In December 2020, approximately 19,409,452 unique accounts posted a comment. This number is an approximation because I used the resevoir sampled data and then multiplied the number of unique accounts commenting in that random sample by the file size of the full data divided by the file size of the sampled data. This, of course, is making several assumptions. One assumption is that the percentage of unique accoutns making comments in the sampled data out of all the comments is similar in the entire data set. This also assumes that the size of each row in the sampled data and the full data are relatively similar. A final assumption, would would still need to be made even if exact counts were found, is that each different Reddit account represents a unique user. Since Reddit is anonymous, it is possible for one user to make many accounts. Another limitation of this analysis is that there are comments which have deleted users. We are not able to count these users in our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8acc0c7",
   "metadata": {},
   "source": [
    "## Best Comment Award"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c9af4",
   "metadata": {},
   "source": [
    "### What was the most upvoted comment on Election Day 2008?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44be41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nov_08 = spark.read.json('hdfs://orion11:13001/RC_2008-11.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8471b1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792310"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nov_08.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fcb367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- downs: long (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- score_hidden: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nov_08.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e65cd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(archived=True, author='[deleted]', author_flair_css_class=None, author_flair_text=None, body='[deleted]', controversiality=0, created_utc='1225497606', distinguished=None, downs=0, edited='false', gilded=0, id='c064gt3', link_id='t3_7ajem', name='t1_c064gt3', parent_id='t3_7ajem', retrieved_on=1425896386, score=1, score_hidden=False, subreddit='reddit.com', subreddit_id='t5_6', ups=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nov_08.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c77930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f36dc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Election Day was on November 4, 2008\n",
    "\n",
    "start_datetime = datetime(2008, 11, 4)\n",
    "end_datetime = datetime(2008, 11, 5)\n",
    "\n",
    "# Convert datetime object to a unix timestamp string which is how timestamps are represented in the schema\n",
    "\n",
    "start_datetime_unix = str(int(time.mktime(start_datetime.timetuple())))\n",
    "end_datetime_unix = str(int(time.mktime(end_datetime.timetuple())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a69d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nov_4_08 = df_nov_08.filter(df_nov_08['created_utc'] >= start_datetime_unix)\n",
    "df_nov_4_08 = df_nov_4_08.filter(df_nov_08['created_utc'] < end_datetime_unix).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a1ea8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37898"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nov_4_08.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2847d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort comments on election day in descending order by upvotes\n",
    "\n",
    "df_nov_4_08 = df_nov_4_08.orderBy(df_nov_4_08[\"ups\"].desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1542996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comment = df_nov_4_08.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "594ce44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(archived=True, author='[deleted]', author_flair_css_class=None, author_flair_text=None, body=\"Dear Rest of The World\\n\\nWe didn't fuck it up\\n\\nSigned,\\n\\nAmerica\", controversiality=0, created_utc='1225858444', distinguished=None, downs=0, edited='false', gilded=0, id='c066uum', link_id='t3_7beo2', name='t1_c066uum', parent_id='t3_7beo2', retrieved_on=1425897518, score=2108, score_hidden=False, subreddit='politics', subreddit_id='t5_2cneq', ups=2108)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a1a14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_nov_4_08 = df_nov_4_08.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ff66c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(archived=True, author='[deleted]', author_flair_css_class=None, author_flair_text=None, body=\"Dear Rest of The World\\n\\nWe didn't fuck it up\\n\\nSigned,\\n\\nAmerica\", controversiality=0, created_utc='1225858444', distinguished=None, downs=0, edited='false', gilded=0, id='c066uum', link_id='t3_7beo2', name='t1_c066uum', parent_id='t3_7beo2', retrieved_on=1425897518, score=2108, score_hidden=False, subreddit='politics', subreddit_id='t5_2cneq', ups=2108),\n",
       " Row(archived=True, author='rockus', author_flair_css_class=None, author_flair_text=None, body='Dear America,\\n\\nCongrats!\\n\\nRegards,\\nRest of the World', controversiality=0, created_utc='1225858825', distinguished=None, downs=0, edited='false', gilded=0, id='c066v41', link_id='t3_7beo2', name='t1_c066v41', parent_id='t1_c066uum', retrieved_on=1425897522, score=1143, score_hidden=False, subreddit='politics', subreddit_id='t5_2cneq', ups=1143),\n",
       " Row(archived=True, author='Hukeshy', author_flair_css_class=None, author_flair_text=None, body='Dear Canada\\r\\n\\r\\nIt will clearly be viewable from Russia.', controversiality=0, created_utc='1225859262', distinguished=None, downs=0, edited='true', gilded=0, id='c066vcu', link_id='t3_7beo2', name='t1_c066vcu', parent_id='t1_c066v83', retrieved_on=1425897525, score=944, score_hidden=False, subreddit='politics', subreddit_id='t5_2cneq', ups=944)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_nov_4_08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e173c",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "The most upvoted comment on Election Day 2008 was:\n",
    "> Dear Rest of The World\n",
    ">\n",
    "> We didn't fuck it up\n",
    ">\n",
    "> Signed,\n",
    ">\n",
    "> America\n",
    "\n",
    "which had 2108 upvotes. However, this comment was made by a deleted user.\n",
    "\n",
    "The most upvoted comment with a known user was:\n",
    "> Dear America,\n",
    "> \n",
    "> Congrats!\n",
    ">\n",
    "> Regards,\n",
    "> Rest of the World\n",
    "\n",
    "which had 1143 upvotes and was made by u/rockus (who is still active on Reddit).\n",
    "\n",
    "The third most upvoted comment was: \n",
    "> Dear Canada\n",
    ">\n",
    "> It will clearly be viewable from Russia.\n",
    "\n",
    "All three of these comments were made on the r/politics subreddit and they all follow a similar format of addressing the comment with \"Dear\".\n",
    "\n",
    "Since the actual top comment was made by a deleted user, we will analyze u/rockus comments in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b991a4a",
   "metadata": {},
   "source": [
    "## Top Comments: See top_comments.ipynb in the GitHub repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c974a7",
   "metadata": {},
   "source": [
    "## Ban Hammer\n",
    "### Based on user activity, which which subreddits have been recently banned?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e4f34",
   "metadata": {},
   "source": [
    "Look for subreddits which go from having many comments to suddenly having none. Maybe in one month they have many comments, then in the next month they suddenly don't have any comments. The question asks about \"recently banned\". I will define this to mean banned in the last month of data which is December 2020. So I will look for a subreddit which had many comments in November 2020 then zero comments in December 2020. This would likely mean the subreddit was banned sometime in November. Alternatively, we could just look at December 2020 comments and try to find a subreddit which was banned in 2020 by using the timestamps. Before a certain time there were many comments to that subreddit then after that time there were none. This would mean the subreddit was likely banned. We can use the reservoir sample data for this question because a banned subreddit would likely have many comments before getting banned so it will likely have comments in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aefa87e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([StructField('subreddit', StringType(), nullable=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c05f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nov_20_res = spark.read.schema(schema).json('hdfs://orion11:13001/RES-RC_2020-11.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1c00e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(subreddit='AssassinsCreedValhala'), Row(subreddit='worldnews')]\n"
     ]
    }
   ],
   "source": [
    "print(df_nov_20_res.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3eaf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dec_20_res = spark.read.schema(schema).json('hdfs://orion11:13001/RES-RC_2020-12.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a829f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nov_count = df_nov_20_res.groupBy('subreddit').count()\n",
    "df_dec_count = df_dec_20_res.groupBy('subreddit').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d76804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nov_count_name = df_nov_count.withColumnRenamed('count', 'nov_count')\n",
    "df_dec_count_name = df_dec_count.withColumnRenamed('count', 'dec_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eba5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full outer join\n",
    "\n",
    "df_join = df_nov_count_name.join(df_dec_count_name, on='subreddit', how='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "887551ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- nov_count: long (nullable = true)\n",
      " |-- dec_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d75b0832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(subreddit='anime', nov_count=14061, dec_count=16185), Row(subreddit='travel', nov_count=878, dec_count=909)]\n"
     ]
    }
   ],
   "source": [
    "print(df_join.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28f84a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = df_join.filter((df_join.nov_count >= 100) & (df_join.dec_count.isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "236603a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "banned_list = df_filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1bc81ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(subreddit='csci040temp', nov_count=1890, dec_count=None),\n",
       " Row(subreddit='DoomerGvng', nov_count=153, dec_count=None),\n",
       " Row(subreddit='Vindicta', nov_count=108, dec_count=None),\n",
       " Row(subreddit='dadswhodidnotwantpets', nov_count=138, dec_count=None),\n",
       " Row(subreddit='ChiefsvsRavensLiveTv', nov_count=165, dec_count=None),\n",
       " Row(subreddit='RadicalChristianity', nov_count=190, dec_count=None),\n",
       " Row(subreddit='YoungPrettyHoes', nov_count=255, dec_count=None),\n",
       " Row(subreddit='megnuttleaks', nov_count=170, dec_count=None),\n",
       " Row(subreddit='donkybooties', nov_count=192, dec_count=None),\n",
       " Row(subreddit='hftyty', nov_count=3230, dec_count=None),\n",
       " Row(subreddit='ismos', nov_count=137, dec_count=None),\n",
       " Row(subreddit='FitnessGuidesSharing', nov_count=455, dec_count=None),\n",
       " Row(subreddit='CallMeCarson', nov_count=275, dec_count=None)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banned_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
