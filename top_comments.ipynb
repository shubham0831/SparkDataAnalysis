{"nbformat_minor": 2, "cells": [{"source": "# Best Comment Award\n## What was the most upvoted comment on Election Day 2016?\n\nIn the site_analysis.ipynb notebook, we answered this same question for Election Day 2008 using the Orion cluster and found a common theme between all of the top comments on Election Day 2008. Now that we are on an Azure Spark cluster, let's look at Election Day 2016.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.sql.types import StructType, StructField, StringType, LongType\nfrom datetime import datetime\nimport time ", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": 7, "cell_type": "code", "source": "# Data was uploaded to Azure block storage\n\nfile_path = 'abfs://dda-2022-12-15t21-04-18-212z@ddasta.dfs.core.windows.net/reddit/2016/RC_2016-11.bz2'\n\n# We are only interested in these columns\n\nschema = StructType([\n    StructField('author', StringType(), nullable=True),\n    StructField('body', StringType(), nullable=True),\n    StructField('score', LongType(), nullable=True),\n    StructField('created_utc', StringType(), nullable=True)\n])\n\ndf_nov_16 = spark.read.schema(schema).json(file_path)\ndf_nov_16.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "71022319"}], "metadata": {"cell_status": {"execute_time": {"duration": 101782.25610351562, "end_time": 1671505160251.804}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 8, "cell_type": "code", "source": "# Election Day was on November 8, 2016\n\nstart_datetime = datetime(2016, 11, 8)\nend_datetime = datetime(2016, 11, 9)\n\n# Convert datetime object to a unix timestamp string which is how timestamps are represented in the schema\n\nstart_datetime_unix = str(int(time.mktime(start_datetime.timetuple())))\nend_datetime_unix = str(int(time.mktime(end_datetime.timetuple())))\n\ndf_nov_8_16 = df_nov_16.filter(df_nov_16['created_utc'] >= start_datetime_unix)\ndf_nov_8_16 = df_nov_8_16.filter(df_nov_16['created_utc'] < end_datetime_unix).cache()\n\ndf_nov_8_16.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "2575570"}], "metadata": {"cell_status": {"execute_time": {"duration": 103783.10791015625, "end_time": 1671505264057.322}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### It's interesting that out of the 71,022,319 comments posted on Reddit in 2016, 2,575,570 were posted on Election Day. That's more than 3.6% of the comments all year posted on one day! For reference 1/365 is only 0.27%!", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 11, "cell_type": "code", "source": "df_nov_8_16.take(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(author='theoriginalharbinger', body=\"Get reduced-cap magazines and you're gtg.\\n\\nWould also suggest buying as much ammo as you're planning on shooting out of state. Cali is implementing a mandate that all ammo go through a background check. [Here's some detail on that](http://www.sacbee.com/news/politics-government/capitol-alert/article88521977.html). My own back-of-the-envelope math indicates this is probably going to cause most ammo sales to be done in bulk and impose anywhere between a 10 to 20% surcharge on the price of ammo.\", score=2, created_utc='1478640324', ups=None), Row(author='wwindexx', body=\"Yes. Yes yes yes. Tax em all. I can't figure out for the life of me why someone who isn't directly affiliated with a church would come up with flimsy excuses on why we shouldn't tax churches. /u/d1rron care to explain why you feel we need to monitor sunday sermons instead of taxing them like everyone else?\", score=5, created_utc='1478640324', ups=None), Row(author='Shanti_Ananda', body='#Thank you patriot!', score=2, created_utc='1478640324', ups=None), Row(author='Grad_ster', body='Papa^Bless', score=1, created_utc='1478640324', ups=None), Row(author='cracklethud', body='haha, oh god yeah it is! ', score=1, created_utc='1478640324', ups=None)]"}], "metadata": {"cell_status": {"execute_time": {"duration": 758.705078125, "end_time": 1671505419405.966}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 18, "cell_type": "code", "source": "from pyspark.sql.functions import *\n\n# Sort comments on election day in descending order by score\n\ndf_nov_8_16 = df_nov_8_16.sort(desc(col(\"score\")))\n\ndf_nov_8_16.take(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(author='RumHam12', body=\"Jesus I'm fuckin retarded \\n\\nEdit: gold for being retarded? So this is what it's like to be in a special Ed program.\", score=20396, created_utc='1478591529', ups=None), Row(author='qvulture', body=\"It's called the Top 40 because that's how many songs early jukeboxes could hold.\", score=17104, created_utc='1478609988', ups=None), Row(author='TooShiftyForYou', body='\"Babe, don\\'t copy Michelle with this one.\"', score=17058, created_utc='1478635133', ups=None), Row(author='st0neh', body='Spacebar.', score=16256, created_utc='1478591418', ups=None), Row(author='Alpha-Trion', body='The Rock and Terry Crews should both star in a buddy cop movie together.', score=15043, created_utc='1478617557', ups=None)]"}], "metadata": {"cell_status": {"execute_time": {"duration": 789.64404296875, "end_time": 1671505693519.258}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## Answer:", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 28, "cell_type": "code", "source": "award_winner = df_nov_8_16.take(1)[0]\nprint(award_winner.author + \" wins for their comment: \\n\\n\" + award_winner.body + \"\\n\\nwith a score of \" + str(award_winner.score))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "RumHam12 wins for their comment: \n\nJesus I'm fuckin retarded \n\nEdit: gold for being retarded? So this is what it's like to be in a special Ed program.\n\nwith a score of 20396"}], "metadata": {"cell_status": {"execute_time": {"duration": 767.06298828125, "end_time": 1671506160836.214}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "Context: https://www.reddit.com/r/pcmasterrace/comments/5brzzu/where_in_the_fuck_is_this_key/d9qte7o/?context=3\n\nI was curioius in what context thsis comment was made, so I did some searching on reddit.com and found this. RumHam had made a post to r/pcmasterrace with a screenshot from a game asking what keyboard key a symbol on in the game was representing. A helpful Reddit user told RumHam it is the spacebar key, and this top comment was the response to that user.\n\nIt's interesting that the top comment on Election Day had nothing to do with the election. Contrast this with the top comments on Election Day 2008 which were found in the site_analysis.ipynb notebook in this same GitHub repo and you will see that the top 3 comments on that day were election related.\n\nOne way to interpret this is that as Reddit grew in users, it no longer focused as much on news and current events and each community took on its own life. So despite the 2016 election being massive news in the United States and across the world, Reddit is a world of its own. Another possible interpretation is just that Reddit users like to upvote funny comments.", "cell_type": "markdown", "metadata": {}}, {"source": "# Top Comments\n## What are the five most-upvoted comments made by u/RumHam12 across the entire dataset. Do they post highly-upvoted comments often or are they a \u201cone hit wonder?\u201d", "cell_type": "markdown", "metadata": {"cell_status": {"execute_time": {"duration": 50.70703125, "end_time": 1671506029271.306}}, "editable": true, "deletable": true}}, {"execution_count": 1, "cell_type": "code", "source": "from pyspark.sql.functions import *\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>79</td><td>application_1671409217564_0101</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-swartz.pkreb2nbje4u5lxwjqb1g1mune.dx.internal.cloudapp.net:8088/proxy/application_1671409217564_0101/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn10-swartz.pkreb2nbje4u5lxwjqb1g1mune.dx.internal.cloudapp.net:30060/node/containerlogs/container_1671409217564_0101_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 259.301025390625, "end_time": 1671558005633.567}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": "winning_author = \"RumHam12\"\n\nmonths = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n\nreddit_directory = 'abfs://dda-2022-12-15t21-04-18-212z@ddasta.dfs.core.windows.net/reddit/'\n\nschema = StructType([\n    StructField('author', StringType(), nullable=True),\n    StructField('body', StringType(), nullable=True),\n    StructField('score', LongType(), nullable=True)\n])", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 53.970947265625, "end_time": 1671558005702.386}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## 2016 top comments", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": "df_2016 = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n\n# Create a data frame with all the 2016 comments\n\nfor month in months:\n    file_path = reddit_directory + \"/2016\" + \"/RC_2016-\" + month + \".bz2\"\n    month_df = spark.read.schema(schema).json(file_path)\n    df_2016 = df_2016.union(month_df)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5314.8408203125, "end_time": 1671558011044.678}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 4, "cell_type": "code", "source": "df_2016 = df_2016.filter(col(\"author\") == winning_author)\ndf_2016 = df_2016.sort(desc(col(\"score\")))\ndf_2016.take(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(author='RumHam12', body=\"Jesus I'm fuckin retarded \\n\\nEdit: gold for being retarded? So this is what it's like to be in a special Ed program.\", score=20396), Row(author='RumHam12', body=\"Plz no\\n\\nEdit: some of you are alright. Don't go on Reddit tomorrow.\", score=2772), Row(author='RumHam12', body=\"I'm going to bed fam. If I wake up and this is on r/all I'm going to run into oncoming traffic.\\n\\n\\nEdit: fuck\", score=2337), Row(author='RumHam12', body='I would just love to be reminded of my undying autism for longer ', score=2197), Row(author='RumHam12', body=\"Good lord I'm so dumb you thought I was kidding \", score=1094), Row(author='RumHam12', body='I spent way too long looking for it ', score=521), Row(author='RumHam12', body=\"Holy shit that's even worse! This thing is a mess.\", score=245), Row(author='RumHam12', body='Lol instead of helping them fix it I just posted it on the Internet and exposed it to the world ', score=127), Row(author='RumHam12', body='They sound like shit ', score=121), Row(author='RumHam12', body='R/lifehacks', score=97)]"}], "metadata": {"cell_status": {"execute_time": {"duration": 894378.0319824219, "end_time": 1671558905438.293}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Analysis\n\nRumHam had three comments with a score between 2000 and 3000 and another comment with a score just above 1000, but their best comment in 2016 by far was their top comment shown above with a score of 20,369. I would consider RumHam to be a one-hit wonder in 2016 because the score of their top comment of the year was an order of magnitude higher than the sccore of their second best comment.\n\nNow we will check 2015 to see their comment score performance in the previous year.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "## 2015 top comments", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": "year = \"2015\"\ndf_2015 = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n\nfor month in months:\n    file_path = reddit_directory + \"/\" + year + \"/RC_\" + year + \"-\" + month + \".bz2\"\n    month_df = spark.read.schema(schema).json(file_path)\n    df_2015 = df_2015.union(month_df)\n    \ndf_2015 = df_2015.filter(col(\"author\") == winning_author)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1265.994873046875, "end_time": 1671558906723.821}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 6, "cell_type": "code", "source": "df_2015.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0"}], "metadata": {"cell_status": {"execute_time": {"duration": 769655.2719726562, "end_time": 1671559676395.314}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Analysis\n\nu/RumHam12 did not make a single comment in 2015. Given this fact, we will assume their account didn't exist in 2015. Now we will check their comment performance in 2017.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "## 2017 top comments", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 7, "cell_type": "code", "source": "year = \"2017\"\ndf_2017 = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n\n# Note that the data set only has the first three months of 2017\nfor month in months[:3]:\n    file_path = reddit_directory + \"/\" + year + \"/RC_\" + year + \"-\" + month + \".bz2\"\n    month_df = spark.read.schema(schema).json(file_path)\n    df_2017 = df_2017.union(month_df)\n\ndf_2017 = df_2017.filter(col(\"author\") == winning_author)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 755.715087890625, "end_time": 1671559677169.251}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 8, "cell_type": "code", "source": "df_2017.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "17"}], "metadata": {"cell_status": {"execute_time": {"duration": 268715.4689941406, "end_time": 1671559945900.466}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 9, "cell_type": "code", "source": "df_2017 = df_2017.sort(desc(col(\"score\")))\ndf_2017.take(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(author='RumHam12', body='Makes sense. Thanks dad!', score=44), Row(author='RumHam12', body=\"That requires me to take more time away from this game and that's not happening take what you get \", score=13), Row(author='RumHam12', body=\"Is there a big skill ceiling? Like it's pretty competitive?\", score=8), Row(author='RumHam12', body='Breath of the wild ', score=5), Row(author='RumHam12', body=\"That's good to know! Thanks!\", score=5), Row(author='RumHam12', body='I can take all the time in the world if the costume is good enough ', score=4), Row(author='RumHam12', body='Cocapoo ', score=2), Row(author='RumHam12', body='Ok that makes way more sense lol. I thought everything I posted had it and I looked like a retard everywhere I went ', score=1), Row(author='RumHam12', body=\"Lol as much as I'd love to submit something Zelda related I don't think this would fly\", score=1), Row(author='RumHam12', body='Oh I can use this! Thanks fam!', score=1)]"}], "metadata": {"cell_status": {"execute_time": {"duration": 262558.43115234375, "end_time": 1671560208473.55}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Analysis\n\nRumHam's comment scores in 2017 were much lower than in 2016. Their highest score comment had a score of only 44. This further confirms RumHam's status as a one-hit wonder commenter.", "cell_type": "markdown", "metadata": {"cell_status": {"execute_time": {"duration": 52.970947265625, "end_time": 1671517873744.109}}, "editable": true, "deletable": true}}, {"source": "## 2018 top comments", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 10, "cell_type": "code", "source": "year = \"2018\"\ndf_2018 = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n\nfor month in months:\n    # Note that we decompressed the files for 2018 and later\n    file_path = reddit_directory + \"/\" + year + \"/RC_\" + year + \"-\" + month + \".json\"\n    month_df = spark.read.schema(schema).json(file_path)\n    df_2018 = df_2018.union(month_df)\n    \ndf_2018 = df_2018.filter(col(\"author\") == winning_author)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1263.363037109375, "end_time": 1671560209752.992}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 11, "cell_type": "code", "source": "df_2018.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "17"}], "metadata": {"cell_status": {"execute_time": {"duration": 558775.3981933594, "end_time": 1671560768567.862}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 12, "cell_type": "code", "source": "df_2018 = df_2018.sort(desc(col(\"score\")))\ndf_2018.take(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(author='RumHam12', body='Can we just make a separate subreddit for this creepy garbage?', score=3), Row(author='RumHam12', body='Indeed', score=3), Row(author='RumHam12', body='This guy gets it ', score=3), Row(author='RumHam12', body='The law ', score=3), Row(author='RumHam12', body='Are you saying zerk is bs?', score=3), Row(author='RumHam12', body=\"If the single motherhood rate explodes right as a government incentive is given out for being a single mother I think it's pretty safe to assume there's a connection there \", score=2), Row(author='RumHam12', body=\"That's like exactly what I was thinking lol\", score=2), Row(author='RumHam12', body='Blackbeard would be dope ', score=2), Row(author='RumHam12', body=\"Wendigo: bound by blood is genuinely the worst movie I have ever seen it's hilarious and I've never heard anyone else even mention it \", score=2), Row(author='RumHam12', body=\"Yea I tired that it'll still like 2.5 gigs \", score=1)]"}], "metadata": {"cell_status": {"execute_time": {"duration": 555650.3459472656, "end_time": 1671561324297.736}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Analysis\n\n2018 was an even weaker year for RumHam's comment scores. Their highest scoring comment only had a score of 3. At this point we can confidently say that RumHam is a one-hit wonder commenter.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "## 2019 top comments", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 13, "cell_type": "code", "source": "year = \"2019\"\ndf_2019 = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n\nfor month in months:\n    file_path = reddit_directory + \"/\" + year + \"/RC_\" + year + \"-\" + month + \".json\"\n    month_df = spark.read.schema(schema).json(file_path)\n    df_2019 = df_2019.union(month_df)\n    \ndf_2019 = df_2019.filter(col(\"author\") == winning_author)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1266.4599609375, "end_time": 1671561325654.887}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 14, "cell_type": "code", "source": "df_2019.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0"}], "metadata": {"cell_status": {"execute_time": {"duration": 890892.07421875, "end_time": 1671562216596.694}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Analysis\n\nRumHam did not make a single comment in 2019. We will assume that are now inactive and will not analyze the 2020 data for comments.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "## Answer\n\nRumHam is a one-hit wonder. Their top 5 comments across the entire dataset are actually just their top 5 comments in 2016 which can be found printed above (I would print them again here, but the notebook was closed and the data was not cached).\n\nNot only was RumHam a one-hit wonder in that their highest scoring comment was an order of magnitude better than their second highest scoring comment, they were also a one-year wonder in that they had several relatively high scoring comments in 2016, but then had much lower scoring comments in other years.", "cell_type": "markdown", "metadata": {"collapsed": true, "editable": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}