{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e12922",
   "metadata": {},
   "source": [
    "# Stream Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e578e82",
   "metadata": {},
   "source": [
    "## Trending Topics\n",
    "\n",
    "Assuming you receive Reddit comments as a live stream, build a Spark streaming application that will consume the stream and determine what topics are trending over a particular window of time.\n",
    "\n",
    "Data was streamed using the streamer.py script which can be found in the scripts directory of this repo.\n",
    "\n",
    "Note: The code in this notebook borrows heavily from the from the spark-examples repo in the USF-BigData GitHub https://github.com/USF-BigData/spark-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb32b0",
   "metadata": {},
   "source": [
    "## Approach 1: Frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0dda0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# The second parameter is the number of seconds between microbatches\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Required to be able to do state updates\n",
    "ssc.checkpoint(\"/bigdata/amkaddoura/checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e30ec1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream comments from orion05\n",
    "sock = ssc.socketTextStream(\"orion05\", 13999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "021ea888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_text(comment_json):\n",
    "    return comment_json[\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04457907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get a set of common \"stop words\" which are common English words that are not trending topics\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e42bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def split_comment(comment):\n",
    "    words = comment.split()\n",
    "    \n",
    "    # Source for removing punctuation: https://stackoverflow.com/a/266162\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.translate(translator) for word in words]\n",
    "    \n",
    "    # Convert all letters to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Source for removing stop words: https://stackoverflow.com/a/5486535\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29e98d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objects = sock.map(lambda line: json.loads(line))\n",
    "text_data = json_objects.flatMap(lambda x: [x.get(\"body\")])\n",
    "filtered_comments = text_data.filter(lambda comment: comment != \"[deleted]\")\n",
    "filtered_comments_words = filtered_comments.flatMap(split_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2399f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def print_frequent_words(rdd):\n",
    "    top_words = sorted(rdd.collect(), key=itemgetter(1), reverse=True)[:10]\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eac089b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = filtered_comments_words.map(lambda x: (x, 1))\n",
    "total_counts = word_counts.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "total_counts.foreachRDD(print_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a96dc64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 33), ('like', 28), ('im', 24), ('get', 18), ('dont', 17), ('one', 17), ('think', 14), ('good', 14), ('way', 13), ('first', 12)]\n",
      "[('like', 17), ('good', 12), ('one', 12), ('would', 11), ('people', 11), ('well', 9), ('get', 8), ('im', 8), ('dont', 8), ('really', 8)]\n",
      "[('', 42), ('like', 32), ('would', 29), ('get', 28), ('one', 25), ('dont', 19), ('really', 18), ('people', 15), ('thats', 14), ('two', 13)]\n",
      "[('like', 15), ('really', 11), ('', 10), ('dont', 10), ('would', 8), ('people', 8), ('music', 7), ('im', 7), ('someone', 7), ('one', 7)]\n",
      "[('like', 23), ('', 16), ('would', 13), ('im', 13), ('dont', 13), ('think', 12), ('really', 10), ('get', 9), ('even', 9), ('people', 9)]\n",
      "[('like', 18), ('get', 12), ('one', 12), ('dont', 9), ('would', 8), ('see', 8), ('say', 7), ('know', 7), ('better', 7), ('', 6)]\n",
      "[('like', 24), ('', 16), ('would', 15), ('one', 15), ('dont', 12), ('get', 10), ('im', 10), ('2', 9), ('much', 9), ('want', 9)]\n",
      "[('like', 28), ('im', 23), ('dont', 23), ('one', 22), ('would', 21), ('get', 21), ('people', 21), ('', 17), ('also', 17), ('even', 17)]\n",
      "[('', 224), ('2010', 18), ('2012', 18), ('change', 12), ('get', 12), ('votes', 12), ('age', 12), ('majority', 12), ('im', 10), ('dont', 10)]\n",
      "[('like', 14), ('good', 9), ('even', 9), ('dont', 9), ('think', 8), ('demand', 8), ('one', 8), ('thats', 7), ('', 6), ('knitting', 6)]\n",
      "[('like', 31), ('people', 23), ('one', 23), ('', 22), ('im', 22), ('would', 18), ('dont', 18), ('well', 18), ('youre', 17), ('even', 16)]\n"
     ]
    }
   ],
   "source": [
    "# Running this will start listening\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "961aa745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 23), ('would', 17), ('ukip', 15), ('even', 14), ('like', 13), ('much', 11), ('see', 11), ('think', 10), ('one', 10), ('dont', 10)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Note: you need the stopSparkContext=False, otherwise your driver will die and you'll have to restart Jupyter\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6d201",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The approach of finding the most frequent words in the comments has potential, but there is too much noise in the comment text data to find the signal of the trending topics. The noise is common words which aren't stop words but aren't topics of discussion. The other problem is that a trending topic is not necessarily a single word. This approach could be improved upon by adding stemming of words, but even then there are too many frequent words which are not trending topics. My next approach will be to look for named entities which are more likely to be trending topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71b5a4",
   "metadata": {},
   "source": [
    "## Approach 2: Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "998c1542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/amkaddoura/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/amkaddoura/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amkaddoura/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27dc8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def find_entities(comments):\n",
    "    entities = []\n",
    "    chunked_comment = ne_chunk(pos_tag(word_tokenize(comments)))\n",
    "    \n",
    "    for subtree in chunked_comment:\n",
    "        if type(subtree) == Tree:\n",
    "            entity_name = \" \".join([word for word, tag in subtree.leaves()])\n",
    "            entities.append(entity_name)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def find_trending_topics(comments):\n",
    "    entities = comments.flatMap(find_entities)\n",
    "    entity_counts = entities.map(lambda entity: (entity, 1)).reduceByKey(lambda x, y: x + y)\n",
    "    trending_topics = entity_counts.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "    \n",
    "    print(trending_topics)\n",
    "        \n",
    "    return trending_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "75c04db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 60)\n",
    "ssc.checkpoint(\"/bigdata/amkaddoura/checkpoint\")\n",
    "sock = ssc.socketTextStream(\"orion05\", 13999)\n",
    "\n",
    "import json\n",
    "\n",
    "def get_text(comment_json):\n",
    "    return comment_json[\"body\"]\n",
    "\n",
    "json_objects = sock.map(lambda line: json.loads(line))\n",
    "text_data = json_objects.flatMap(lambda row: [row.get(\"body\")])\n",
    "filtered_comments = text_data.filter(lambda comment: comment != \"deleted\")\n",
    "\n",
    "filtered_comments.foreachRDD(lambda rdd: find_trending_topics(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ade27954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sorry', 25), ('Israel', 20), ('Great', 16), ('UKIP', 16), ('America', 12), ('Wow', 11), ('Christmas', 9), ('Lib Dems', 9), ('THE', 9), ('Mexico', 9)]\n",
      "[('Israel', 53), ('US', 46), ('Fnatic', 36), ('Obama', 31), ('Reddit', 30), ('OP', 29), ('Canada', 25), ('Great', 25), ('Sorry', 24), ('Bob', 24)]\n",
      "[('Israel', 46), ('US', 40), ('Sorry', 40), ('Reddit', 29), ('American', 27), ('Canada', 24), ('ImGoingToHellForThis', 23), ('Great', 23), ('English', 21), ('Texas', 21)]\n",
      "[('Israel', 43), ('American', 43), ('US', 42), ('Sorry', 29), ('Reddit', 28), ('Google', 27), ('Obama', 26), ('OP', 26), ('Great', 23), ('Good', 23)]\n",
      "[('US', 49), ('Sorry', 43), ('Good', 25), ('Israel', 24), ('Reddit', 23), ('America', 22), ('Obama', 22), ('OP', 21), ('Wow', 20), ('Haha', 20)]\n",
      "[('Reddit', 48), ('Israel', 38), ('US', 33), ('Sorry', 28), ('OP', 28), ('America', 27), ('God', 24), ('Good', 24), ('American', 23), ('Ah', 21)]\n",
      "[('Fnatic', 40), ('US', 36), ('Sorry', 29), ('Great', 24), ('Please', 24), ('Reddit', 24), ('OP', 23), ('Israel', 23), ('TPA', 20), ('Obama', 19)]\n",
      "[('US', 55), ('Israel', 35), ('OP', 35), ('Sorry', 35), ('Reddit', 34), ('Fnatic', 34), ('American', 28), ('Jesus', 27), ('Canada', 26), ('TPA', 26)]\n",
      "[('US', 45), ('Israel', 41), ('Sorry', 33), ('Please', 30), ('God', 29), ('Jesus', 28), ('Good', 25), ('Reddit', 25), ('Thanks', 21), ('OP', 21)]\n",
      "[('US', 46), ('Israel', 40), ('Sorry', 39), ('Reddit', 34), ('Please', 27), ('Canada', 26), ('OP', 24), ('Good', 23), ('Great', 21), ('Haha', 21)]\n",
      "[('US', 48), ('Sorry', 38), ('Israel', 35), ('Reddit', 26), ('American', 25), ('OP', 24), ('America', 22), ('Wow', 20), ('Good', 19), ('God', 19)]\n",
      "[('Israel', 65), ('US', 46), ('Reddit', 45), ('America', 37), ('Sorry', 30), ('Canada', 20), ('English', 19), ('Good', 19), ('Dude', 19), ('American', 19)]\n",
      "[('US', 37), ('Sorry', 36), ('American', 36), ('Israel', 29), ('Reddit', 28), ('Wow', 28), ('Good', 27), ('OP', 25), ('CLG', 24), ('Hey', 22)]\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "589de8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734381d6",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b4837",
   "metadata": {},
   "source": [
    "The named entities approach to finding trending topics is much more accurate than the frequent words approach. I streamed data from December 2012 and I see Christmas as a trending topic several times which seems accurate. The 2012 Israeli operation in the Gaza Strip was also around this time and there were heightened tensions in the conflict so it makes sense to see Palestine, Israel, and Hamas as trending topics. Obama is also trending topic and this makes sense giving that he won the 2012 Presidential Election a month prior. America and USA are also common trending topics which makes sense because Reddit is an American website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
